---
sidebar_position: 2
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# How to Run Periflow Container?


## Introduction

Periflow Container enables you to effortlessly deploy your own AI model checkpoint on your local machine.
This tutorial will guide you through the process of running Periflow Container.


## Prerequisites

Before you begin, make sure you have to signup to Periflow Cloud.
In the next step, you will need to pull the Docker container image.
You can find instructions on how to do this in the guide on the [Periflow Cloud webpage](PERIFLOW CLOUD WEBPAGE URL).

:::info
### Converting Hugging Face Checkpoint (Optional)

If you have a Hugging Face checkpoint and want to convert it to a PeriFlow-compatible format, you need to install the package with the following command:

```sh
pip install periflow-client[mllib]
```

:::


## Step 1. Converting Hugging Face Checkpoint (Optional)

If you want to serve your AI model with PeriFlow, you'll need to convert it to the PeriFlow-compatible format.
Here's how you can easily convert a Hugging Face model checkpoint to the PeriFlow format:

1. Open a terminal shell (if not already open).
2. Use the following command to convert the Hugging Face model checkpoint to the PeriFlow-compatible format. Replace `$MODEL_NAME_OR_PATH`, `$OUTPUT_DIR`, and `$DATA_TYPE` with the actual values you want.

```sh
pf checkpoint convert \
    --model-name-or-path $MODEL_NAME_OR_PATH \
    --output-dir $OUTPUT_DIR \
    --data-type $DATA_TYPE
```

:::info
You have two options for `$MODEL_NAME_OR_PATH`:

- Enter the local checkpoint path.
- Use the Hugging Face model name (e.g., `gpt2`, `EleutherAI/gpt-j-6b`).

For the `$DATA_TYPE` parameter, you can choose from three options:

- `fp16`: 16-bit floating-point format.
- `fp32`: 32-bit floating-point format.
- `bf16`: bfloat16 format.
:::

3. After executing the above command, the following files will be created at $OUTPUT_DIR:
    - `model.h5`: The converted model checkpoint file with HDF5 format.
    - `tokenizer.json`: Tokenizer file used for tokenizing/detokenizing the model inputs/ouputs.
    - `attr.yaml`: The checkpoint attributes file containing the model configurations and generation configurations.

:::caution
PeriFlow CLI will try to configure `tokenizer.json` and `attr.yaml` automatically, but the automatic configuration might fail in the following cases:

- If the model does support the [**Fast tokenizer**](https://huggingface.co/learn/nlp-course/chapter6/3), which is compatible with PeriFlow, `tokenizer.json` will not be created.
- If the model config or generation config published to Hugging Face does not contain the required information, the value will be left blank (marked with "FILL ME") in `attr.yaml`.
- If there is a conflict between the model config and the generation config published to the Hugging Face, the value will be left blank (marked with "FILL ME") in `attr.yaml`.
:::

After completing this step, your Hugging Face model checkpoint will be converted to the PeriFlow-compatible format, and you'll be able to use it with PeriFlow seamlessly.


## Step 2. Run PeriFlow Container with Checkpoint

If the PeriFlow-compatible checkpoint is ready, here's the instructions to run your model with PeriFlow Container.
PeriFlow Container supports the following model types:

- GPT
- Llama (inlcuding Llama 2)
- MPT
- Falcon
- BLOOM
- GPT-J
- GPT-NeoX
- OPT
- T5 (including T5 v1.1)
- BlenderBot


### Run PeriFlow Container
```sh
docker run --gpus=$NUMBER_OF_GPUS -v $LOCAL_CKPT_PATH:/model \
 --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
"/root/launcher [OPTIONS]"
```

### Options
#### General Options
| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--version` | - | Print PeriFlow Container version. | - | ❌ |
| `--help` | - | Print PeriFlow Container help message. | - | ❌ |

#### Launch Options
| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--web-server-port` | INT | Web server port. | - | ✅ |
| `--metrics-port` | INT | Prometheus metrics export port. | 8281 | ❌ |
| `--tokenizer-file-path` | TEXT | Absolute path of tokenizer file. | - | ❌ |
| `--bad-stop-file-path` | TEXT | Json file path that contains stop/bad words/tokens. For more information about this option, please refer to [this document](/cli/deployment/create#summary). | - | ❌ |
| `--num-request-threads` | INT | Thread pool size for handling HTTP requests. | 4 | ❌ |
| `--timeout-microseconds` | INT | Servier-side timeout for client requests, in microseconds. | 0 (no timeout) | ❌ |
| `--ignore-nan-error` | - | Ignore Nan error. | - | ❌ |
| `--num-devices, -d` | INT | Number of devices to use in tensor parallelism degree. This value should not be larger than the number of available GPUs. | 1 | ❌ |
| `--max-token-count` | INT | Max number of tokens that can be processed at once. | 8192 | ❌ |
| `--max-batch-size` | INT | Max number of items that can be processed at once. | 256 | ❌ |
| `--dtype` | CHOICE: [bf16, fp16, fp32] | Checkpoint data type. Choose one of {fp16\|bf16\|fp32} | fp16 | ❌ |
| `--ckpt-path` | TEXT | Absolute path of model checkpoint. If not specified, use uninitialized (garbage) values for model parameters. | - | ❌ |


#### Model Options

##### Decoder Only Model Options
The following tables are the options for decoder only models: GPT, Llama, MPT, Falcon, BLOOM, GPT-J, GPT-NeoX, OPT

| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--num-layers` | INT | Number of layers. | - | ✅ |
| `--num-heads` | INT | Number of attention heads. | - | ✅ |
| `--num-kv-heads` | INT | Number of kv heads. If this value is different from `num-heads`, it implies grouped query attention. | `num-heads` | ❌ |
| `--head-size` | INT | Attention head size | - | ✅ |
| `--rotary-dim` | INT | Rotary position embedding dim size. Only for models with RoPE. | - | <ul><li> Llama, Falcon, GPT-J, GPT-NeoX: ✅ </li><li> GPT, MPT, BLOOM, OPT: ❌ </li></ul> |
| `--hidden-size` | INT | Hidden size of model. | `num-heads * head-size` | ❌ |
| `--ff-intermediate-size` | INT | Feed forward intermediate size. Only for Llama model. | `hidden-size * 4` | ❌ |
| `--max-length` | INT | Max length, including both prompt tokens and generated tokens. | - | ✅ |
| `--vocab-size` | INT | Vocabulary size. | - | ✅ |
| `--eos-token` | TEXT | Comma separated list of eos tokens. | - | ✅ |
| `--clip-qkv` | FLOAT | Clipping value for attention qkv. Only for MPT models. | 0.f | ❌ |
| `--scale-attn-by-inverse-layer-idx` | - | Whether to additionally scale attention weights by `1 / layer_idx + 1`. If not given, do not scale. | - | ❌ |
| `--layer-norm-eps` | FLOAT | 'epsilon' value of (rms) layer norm. | 1e-5 | ❌ |


##### Encoder Decoder Model Options
The following tables are the options for encoder decoder models: T5, BlenderBot

| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--num-encoder-layers` | INT | Number of encoder layers. | - | ✅ |
| `--num-decoder-layers` | INT | Number of decoder layers. | - | ✅ |
| `--num-heads` | INT | Number of attention heads. | - | ✅ |
| `--head-size` | INT | Attention head size | - | ✅ |
| `--hidden-size` | INT | Hidden size of model. | `num-heads * head-size` | ❌ |
| `--ff-intermediate-size` | INT | Feed forward intermediate size. | `hidden-size * 4` | ✅ |
| `--max-input-length` | INT | Max input token length. | - | ✅ |
| `--max-output-length` | INT | Max output token length. | - | ✅ |
| `--num-pos-emb-buckets` | INT | Number of position embedding buckets. Only for models with T5-style relative pos embedding. | - | <ul><li> T5: ✅ </li><li> BlenderBot: ❌ </li></ul> |
| `--max-pos-distance` | INT | Max position distance. Only for models with T5-style relative pos embedding. | - | <ul><li> T5: ✅ </li><li> BlenderBot: ❌ </li></ul> |
| `--vocab-size` | INT | Vocabulary size. | - | ✅ |
| `--eos-token` | TEXT | Comma separated list of eos tokens. | - | ✅ |
| `--decoder-start-token` | INT | Decoder start token. | - | ✅ |
| `--layer-norm-eps` | FLOAT | 'epsilon' value of (rms) layer norm. | 1e-5 | ❌ |


### Example
The following examples show how to launch Periflow Container for each model type.

:::caution
The following instructions assumed that `model.h5` file (and `tokenizer.json`) is in $LOCAL_CKPT_PATH.
:::


<Tabs>
<TabItem value="GPT" label="GPT">

This is an example how to launch [GPT2-XL model](https://huggingface.co/gpt2-xl) with fp16.
```sh
# Run GPT2-XL model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type gpt\
    --num-layers 48 --num-heads 25 --head-size 64 \
    --max-length 1024 --vocab-size 50257 --eos-token 50256 \
  "
```

</TabItem>
<TabItem value="Llama" label="Llama">

This is an example how to launch [Meta Llama-2-70b-hf model](https://huggingface.co/meta-llama/Llama-2-70b-hf).
```sh
# Run Llama-2-70b-hf model with float16 data type and 4 gpus (tensor parallel).

$ docker run --gpus=4 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type llama --num-devices 4 \
    --num-layers 80 --num-heads 64 --num-kv-heads 8 --head-size 128 \
    --rotary-dim 128 --ff-intermediate-size 28672 \
    --max-length 4096 --vocab-size 32000 --eos-token 2 \
  "
```

</TabItem>
<TabItem value="MPT">

This is an example how to launch [MosaicML MPT-7B-instruct model](https://www.mosaicml.com/blog/mpt-7b).
```sh
# Run MPT-7B model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type mpt \
    --num-layers 32 --num-heads 32 --head-size 128 \
    --max-length 2048 --vocab-size 50432 --eos-token 0 \
  "
```

</TabItem>
<TabItem value="Falcon">

This is an example how to launch [TII UAE Falcon-7B-Instruct model](https://huggingface.co/tiiuae/falcon-7b-instruct).
```sh
# Run Falcon-7B-Instruct model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type falcon-7b \
    --num-layers 32 --num-heads 71 --num-kv-heads 1 --head-size 64 \
    --rotary-dim 64 
    --max-length 2048 --vocab-size 65024 --eos-token 11 \
  "
```

</TabItem>

<TabItem value="BLOOM">

This is an example how to launch [BigSicence BLOOMZ-7B1 model](https://huggingface.co/bigscience/bloomz-7b1).
```sh
# Run BLOOMZ-7B1 model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type bloom \
    --num-layers 30 --num-heads 32 --head-size 128 \
    --max-length 2048 --vocab-size 250880 --eos-token 2 \
  "
```

</TabItem>

<TabItem value="GPT-J">

This is an example how to launch [EleutherAI GPT-J-6B model](https://huggingface.co/EleutherAI/gpt-j-6b).
```sh
# Run GPT-J-6B model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type gpt-j \
    --num-layers 28 --num-heads 16 --head-size 256 \
    --rotary-dim 64
    --max-length 2048 --vocab-size 50400 --eos-token 50256 \
  "
```

</TabItem>

<TabItem value="GPT-NeoX">

This is an example how to launch [Databricks Dolly-V2-12B model](https://huggingface.co/databricks/dolly-v2-12b).
```sh
# Run Dolly-V2-12B model with float16 data type and 2 gpus (tensor parallel).

$ docker run --gpus=2 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type gpt-neox --num-devices 2 \
    --num-layers 36 --num-heads 40 --head-size 128 \
    --rotary-dim 32
    --max-length 2048 --vocab-size 50280 --eos-token 0 \
  "
```

</TabItem>

<TabItem value="OPT">

This is an example how to launch [Facebook OPT-13B model](https://huggingface.co/facebook/opt-13b).
```sh
# Run Dolly-V2-12B model with float16 data type and 2 gpus (tensor parallel).

$ docker run --gpus=2 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type opt --num-devices 2 \
    --num-layers 40 --num-heads 40 --head-size 128 \
    --max-length 2048 --vocab-size 50272 --eos-token 2 \
  "
```

</TabItem>

<TabItem value="T5">

This is an example how to launch [T5-Base model](https://huggingface.co/t5-base).
```sh
# Run T5-Base model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type t5 \
    --num-encoder-layers 12 --num-decoder-layers 12 --num-heads 12 --head-size 64 \
    --hidden-size 768 --ff-intermediate-size 3072 --max-input-length 128 --max-output-length 128 \
    --num-pos-emb-buckets 32 --max-pos-distance 128
    --vocab-size 32128 --eos-token 1 --decoder-start-token 0 \
  "
```

</TabItem>

<TabItem value="T5 v1.1">

This is an example how to launch [Google Flan-T5-Xl model](https://huggingface.co/google/flan-t5-xl).
```sh
# Run Flan-T5-Xl model with float16 data type and 2 gpus (tensor parallel).

$ docker run --gpus=2 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type t5-v1_1 --num-devices 2 \
    --num-encoder-layers 24 --num-decoder-layers 24 --num-heads 32 --head-size 64 \
    --hidden-size 2048 --ff-intermediate-size 5120 --max-input-length 512 --max-output-length 512 \
    --num-pos-emb-buckets 32 --max-pos-distance 128
    --vocab-size 32128 --eos-token 1 --decoder-start-token 0 \
  "
```

</TabItem>

<TabItem value="BlenderBot">

This is an example how to launch [Facebook BlenderBot-3B model](https://huggingface.co/facebook/blenderbot-3B).
```sh
# Run BlenderBot-3B model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host --ipc=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokoenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \ 
    --ignore-nan-error --dtype fp16 --model-type blenderbot \
    --num-encoder-layers 2 --num-decoder-layers 24 --num-heads 32 --head-size 80 \
    --hidden-size 2560 --ff-intermediate-size 10240 --max-input-length 128 --max-output-length 128 \
    --vocab-size 8008 --eos-token 2 --decoder-start-token 1 \
  "
```

</TabItem>

</Tabs>


## Step 3. Inference with Running PeriFlow Container

Now, we can send inference requests to the running PeriFlow Container.
For information on all parameters that can be used in an inference request, please refer to [this document](/sdk/api/completion#options).

### Examples

<Tabs>
<TabItem value="Bash">

```sh
# Inference request to Running PeriFlow Container using `curl` command.

$ curl -X POST $SERVER_IP:$WEB_SERVER_PORT/v1/completions \ 
  -H "Content-Type: application/json" \
  -d '{"prompt": "Python is a popular", "min_tokens": 20, "max_tokens": 30, \
       "top_k": 32, "top_p": 0.8, "n": 3, "no_repeat_ngram": 3, \ 
       "ngram_repetition_penalty": 1.75}'
```

</TabItem>

<TabItem value="Python">

```python
# Inference request to Running PeriFlow Container using python requests.

import requests
import json

output = requests.post(
    url=url,
    headers=headers,
    json={
        "prompt": "Python is a popular",
        "min_tokens": 20,
        "max_tokens": 30,
        "top_k": 32,
        "top_p": 0.8,
        "n": 3,
        "no_repeat_ngram": 3,
        "ngram_repetition_penalty": 1.75,
    },
)
print(output.json())
```

</TabItem>
</Tabs>