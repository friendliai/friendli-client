---
sidebar_position: 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# How to Run PeriFlow Container?

## Introduction

PeriFlow Container enables you to effortlessly deploy your generative AI model on your machine.
This tutorial will guide you through the process of running PeriFlow Container.
Current version of PeriFlow Container supports all major generative language models, including LLaMA 2, Falcon, MPT, and Dolly.

Supported model types are:

- GPT
- Llama (inlcuding Llama 2, Alpaca, Vicuna, and more)
- MPT
- Falcon
- BLOOM
- GPT-J
- GPT-NeoX (including Pythia, Dolly, and more)
- OPT
- T5 (including T5 v1.1, FLAN, and more)
- BlenderBot

If your model does not belong to one of the above model types, please ask for support by sending an email to [Support](mailto:support@friendli.ai).

## Prerequisites

Before you begin, make sure you have to signup to PeriFlow Container service.
You can use PeriFlow Container trial for a period of two weeks free of charge.
In the next step, you will need to pull the Docker container image.
You can find instructions on how to do this in the guide on the [PeriFlow Container webpage](https://container.periflow.ai).

If you're new to PeriFlow Container and you don't yet have an account, sign up to the [preview](https://preview.periflow.ai/container) and we'll let you know as soon as we open up new account.

:::info
### Requirement for converting Hugging Face model checkpoints (Optional)

To convert a Hugging Face checkpoint to a PeriFlow-compatible format, you need to install the package with the following command:

```sh
pip install periflow-client[mllib]
```

:::


## Step 1. Converting Hugging Face Checkpoint (Optional)

To serve your AI model with PeriFlow, you'll need to convert it to a PeriFlow-compatible format.
Here's how you can easily convert a Hugging Face model checkpoint to the PeriFlow format:

1. Open a terminal shell (if not already open).
2. Use the following command to convert the Hugging Face model checkpoint to the PeriFlow-compatible format. Replace `$MODEL_NAME_OR_PATH`, `$OUTPUT_DIR`, and `$DATA_TYPE` with the actual values you want.

```sh
pf checkpoint convert \
  --model-name-or-path $MODEL_NAME_OR_PATH \
  --output-dir $OUTPUT_DIR \
  --data-type $DATA_TYPE
```

:::info
You have two options for `$MODEL_NAME_OR_PATH`:

- Enter the local checkpoint path.
- Use the Hugging Face model name hosted by the Hugging Face Hub (e.g., `gpt2`, `EleutherAI/gpt-j-6b`, or `meta-llama/Llama-2-7b-hf`).

For the `$DATA_TYPE` parameter, you can choose from three options:

- `fp16`: 16-bit floating-point format.
- `bf16`: bfloat16 format.
- `fp32`: 32-bit floating-point format.
:::

3. After executing the above command, the following files will be created at $OUTPUT_DIR:
    - `model.h5`: The converted model checkpoint file with HDF5 format.
    - `tokenizer.json`: Tokenizer file used for tokenizing/detokenizing the model inputs/ouputs.
    - `attr.yaml`: The checkpoint attributes file containing the model configurations and generation configurations.

:::caution
PeriFlow CLI will try to configure `tokenizer.json` and `attr.yaml` automatically, but the automatic configuration might fail in the following cases:

- If the tokenizer is not compatible with [**Fast Tokenizer**](https://huggingface.co/docs/transformers/fast_tokenizers), `tokenizer.json` will not be created and you cannot use tokenization functionality of PeriFlow Container.
- If the model config or generation config does not contain the required information, the value will be left blank (marked with "FILL ME") in `attr.yaml`.
- If there is a conflict between the model config and the generation config, the value will be left blank (marked with "FILL ME") in `attr.yaml`.
:::

After completing this step, your Hugging Face model checkpoint will be converted to the PeriFlow-compatible format, and you'll be able to use it with PeriFlow seamlessly.


## Step 2. Run PeriFlow Container with Checkpoint

Here's the instructions to run your model with PeriFlow Container:


### Run PeriFlow Container
```sh
docker run --gpus $GPU_ENUMERATION -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher [OPTIONS]"
```

By running the above command, you will have a running Docker container that exports a HTTP endpoint for handling inference requests.
You can customize the command if you are familiar with Docker.
For example, you can connect the container via virtual network interface instead of specifying `--network=host`, which grants access to all interfaces from the host.

### Options
#### General Options
| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--version` | - | Print PeriFlow Container version. | - | ❌ |
| `--help` | - | Print PeriFlow Container help message. | - | ❌ |

#### Launch Options
| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--web-server-port` | INT | Web server port. | - | ✅ |
| `--metrics-port` | INT | Prometheus metrics export port. | 8281 | ❌ |
| `--tokenizer-file-path` | TEXT | Absolute path of tokenizer file. | - | ❌ |
| `--bad-stop-file-path` | TEXT | Json file path that contains stop/bad words/tokens. For more information about this option, please refer to [this document](/cli/deployment/create#summary). | - | ❌ |
| `--num-request-threads` | INT | Thread pool size for handling HTTP requests. | 4 | ❌ |
| `--timeout-microseconds` | INT | Servier-side timeout for client requests, in microseconds. | 0 (no timeout) | ❌ |
| `--ignore-nan-error` | - | If specified, ignore NaN error. Otherwise, respond with a 400 status code if NaN values are detected while processing a request. | - | ❌ |
| `--num-devices, -d` | INT | Number of devices to use in tensor parallelism degree. This value should not be larger than the number of available GPUs. | 1 | ❌ |
| `--max-token-count` | INT | Max number of tokens that can be processed in a batch. | 8192 | ❌ |
| `--max-batch-size` | INT | Max number of sequences that can be processed in a batch. | 256 | ❌ |
| `--dtype` | CHOICE: [bf16, fp16, fp32] | Checkpoint data type. Choose one of {fp16\|bf16\|fp32}. Must agree with the data type of the model checkpoint being used. | fp16 | ❌ |
| `--ckpt-path` | TEXT | Absolute path of model checkpoint. If not specified, use uninitialized (garbage) values for model parameters. | - | ❌ |


#### Model Options

##### Decoder Only Model Options
The following tables are the options for decoder only models: GPT, Llama, MPT, Falcon, BLOOM, GPT-J, GPT-NeoX, OPT

| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--num-layers` | INT | Number of layers. | - | ✅ |
| `--num-heads` | INT | Number of attention heads. | - | ✅ |
| `--num-kv-heads` | INT | Number of kv heads. If this value is different from `num-heads`, it implies grouped query attention. | `num-heads` | ❌ |
| `--head-size` | INT | Attention head size | - | ✅ |
| `--rotary-dim` | INT | Rotary position embedding dim size. Only for models with RoPE. | - | <ul><li> Llama, Falcon, GPT-J, GPT-NeoX: ✅ </li><li> GPT, MPT, BLOOM, OPT: ❌ </li></ul> |
| `--hidden-size` | INT | Hidden size of model. | `num-heads * head-size` | ❌ |
| `--ff-intermediate-size` | INT | Feed forward intermediate size. Only for Llama model. | `hidden-size * 4` | ❌ |
| `--max-length` | INT | Max length, including both prompt tokens and generated tokens. | - | ✅ |
| `--vocab-size` | INT | Vocabulary size. | - | ✅ |
| `--eos-token` | TEXT | Comma separated list of eos tokens. | - | ✅ |
| `--clip-qkv` | FLOAT | Clipping value for attention qkv. Only for MPT models. | 0.f | ❌ |
| `--scale-attn-by-inverse-layer-idx` | - | Whether to additionally scale attention weights by `1 / layer_idx + 1`. If not given, do not scale. | - | ❌ |
| `--layer-norm-eps` | FLOAT | 'epsilon' value of (rms) layer norm. | 1e-5 | ❌ |


##### Encoder Decoder Model Options
The following tables are the options for encoder decoder models: T5, BlenderBot

| Options | Type | Summary | Default | Required |
|---------|------|---------|---------|----------|
| `--num-encoder-layers` | INT | Number of encoder layers. | - | ✅ |
| `--num-decoder-layers` | INT | Number of decoder layers. | - | ✅ |
| `--num-heads` | INT | Number of attention heads. | - | ✅ |
| `--head-size` | INT | Attention head size | - | ✅ |
| `--hidden-size` | INT | Hidden size of model. | `num-heads * head-size` | ❌ |
| `--ff-intermediate-size` | INT | Feed forward intermediate size. | `hidden-size * 4` | ❌ |
| `--max-input-length` | INT | Max input token length. | - | ✅ |
| `--max-output-length` | INT | Max output token length. | - | ✅ |
| `--num-pos-emb-buckets` | INT | Number of position embedding buckets. Only for models with T5-style relative pos embedding. | - | <ul><li> T5: ✅ </li><li> BlenderBot: ❌ </li></ul> |
| `--max-pos-distance` | INT | Max position distance. Only for models with T5-style relative pos embedding. | - | <ul><li> T5: ✅ </li><li> BlenderBot: ❌ </li></ul> |
| `--vocab-size` | INT | Vocabulary size. | - | ✅ |
| `--eos-token` | TEXT | Comma separated list of eos tokens. | - | ✅ |
| `--decoder-start-token` | INT | Decoder start token. | - | ✅ |
| `--layer-norm-eps` | FLOAT | 'epsilon' value of (rms) layer norm. | 1e-5 | ❌ |


### Example
The following examples show how to launch PeriFlow Container for each model type.

:::caution
The following instructions assumed that `model.h5` file (and `tokenizer.json`) is in $LOCAL_CKPT_PATH.
:::


<Tabs>
<TabItem value="GPT" label="GPT">

This is an example how to launch [GPT2-XL model](https://huggingface.co/gpt2-xl) with fp16.
```sh
# Run GPT2-XL model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type gpt\
    --num-layers 48 --num-heads 25 --head-size 64 \
    --max-length 1024 --vocab-size 50257 --eos-token 50256 \
  "
```

</TabItem>
<TabItem value="Llama" label="Llama">

This is an example how to launch [Meta Llama-2-70b-hf model](https://huggingface.co/meta-llama/Llama-2-70b-hf).
```sh
# Run Llama-2-70b-hf model with float16 data type and 4 gpus (tensor parallel).

$ docker run --gpus=4 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type llama --num-devices 4 \
    --num-layers 80 --num-heads 64 --num-kv-heads 8 --head-size 128 \
    --rotary-dim 128 --ff-intermediate-size 28672 \
    --max-length 4096 --vocab-size 32000 --eos-token 2 \
  "
```

</TabItem>
<TabItem value="MPT">

This is an example how to launch [MosaicML MPT-7B-instruct model](https://www.mosaicml.com/blog/mpt-7b).
```sh
# Run MPT-7B model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type mpt \
    --num-layers 32 --num-heads 32 --head-size 128 \
    --max-length 2048 --vocab-size 50432 --eos-token 0 \
  "
```

</TabItem>
<TabItem value="Falcon">

This is an example how to launch [TII UAE Falcon-7B-Instruct model](https://huggingface.co/tiiuae/falcon-7b-instruct).
```sh
# Run Falcon-7B-Instruct model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type falcon-7b \
    --num-layers 32 --num-heads 71 --num-kv-heads 1 --head-size 64 \
    --rotary-dim 64
    --max-length 2048 --vocab-size 65024 --eos-token 11 \
  "
```

</TabItem>

<TabItem value="BLOOM">

This is an example how to launch [BigSicence BLOOMZ-7B1 model](https://huggingface.co/bigscience/bloomz-7b1).
```sh
# Run BLOOMZ-7B1 model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type bloom \
    --num-layers 30 --num-heads 32 --head-size 128 \
    --max-length 2048 --vocab-size 250880 --eos-token 2 \
  "
```

</TabItem>

<TabItem value="GPT-J">

This is an example how to launch [EleutherAI GPT-J-6B model](https://huggingface.co/EleutherAI/gpt-j-6b).
```sh
# Run GPT-J-6B model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type gpt-j \
    --num-layers 28 --num-heads 16 --head-size 256 \
    --rotary-dim 64
    --max-length 2048 --vocab-size 50400 --eos-token 50256 \
  "
```

</TabItem>

<TabItem value="GPT-NeoX">

This is an example how to launch [Databricks Dolly-V2-12B model](https://huggingface.co/databricks/dolly-v2-12b).
```sh
# Run Dolly-V2-12B model with float16 data type and 2 gpus (tensor parallel).

$ docker run --gpus=2 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type gpt-neox --num-devices 2 \
    --num-layers 36 --num-heads 40 --head-size 128 \
    --rotary-dim 32
    --max-length 2048 --vocab-size 50280 --eos-token 0 \
  "
```

</TabItem>

<TabItem value="OPT">

This is an example how to launch [Facebook OPT-13B model](https://huggingface.co/facebook/opt-13b).
```sh
# Run Dolly-V2-12B model with float16 data type and 2 gpus (tensor parallel).

$ docker run --gpus=2 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type opt --num-devices 2 \
    --num-layers 40 --num-heads 40 --head-size 128 \
    --max-length 2048 --vocab-size 50272 --eos-token 2 \
  "
```

</TabItem>

<TabItem value="T5">

This is an example how to launch [T5-Base model](https://huggingface.co/t5-base).
```sh
# Run T5-Base model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type t5 \
    --num-encoder-layers 12 --num-decoder-layers 12 --num-heads 12 --head-size 64 \
    --hidden-size 768 --ff-intermediate-size 3072 --max-input-length 128 --max-output-length 128 \
    --num-pos-emb-buckets 32 --max-pos-distance 128
    --vocab-size 32128 --eos-token 1 --decoder-start-token 0 \
  "
```

</TabItem>

<TabItem value="T5 v1.1">

This is an example how to launch [Google Flan-T5-Xl model](https://huggingface.co/google/flan-t5-xl).
```sh
# Run Flan-T5-Xl model with float16 data type and 2 gpus (tensor parallel).

$ docker run --gpus=2 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type t5-v1_1 --num-devices 2 \
    --num-encoder-layers 24 --num-decoder-layers 24 --num-heads 32 --head-size 64 \
    --hidden-size 2048 --ff-intermediate-size 5120 --max-input-length 512 --max-output-length 512 \
    --num-pos-emb-buckets 32 --max-pos-distance 128
    --vocab-size 32128 --eos-token 1 --decoder-start-token 0 \
  "
```

</TabItem>

<TabItem value="BlenderBot">

This is an example how to launch [Facebook BlenderBot-3B model](https://huggingface.co/facebook/blenderbot-3B).
```sh
# Run BlenderBot-3B model with float16 data type and 1 gpu.

$ docker run --gpus=1 -v $LOCAL_CKPT_PATH:/model \
  --network=host $PERIFLOW_CONTAINER_IMAGE /bin/bash -c \
  "/root/launcher \
    --web-server-port 6000 --tokenizer-file-path /model/tokenizer.json --ckpt-path /model/model.h5 \
    --ignore-nan-error --dtype fp16 --model-type blenderbot \
    --num-encoder-layers 2 --num-decoder-layers 24 --num-heads 32 --head-size 80 \
    --hidden-size 2560 --ff-intermediate-size 10240 --max-input-length 128 --max-output-length 128 \
    --vocab-size 8008 --eos-token 2 --decoder-start-token 1 \
  "
```

</TabItem>

</Tabs>


## Step 3. Inference with Running PeriFlow Container

Now, we can send inference requests to the running PeriFlow Container.
For information on all parameters that can be used in an inference request, please refer to [this document](/sdk/api/completion#options).

### Examples

<Tabs>
<TabItem value="Bash">

```sh
# Inference request to Running PeriFlow Container using `curl` command.

$ curl -X POST $SERVER_IP:$WEB_SERVER_PORT/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Python is a popular", "min_tokens": 20, "max_tokens": 30,
       "top_k": 32, "top_p": 0.8, "n": 3, "no_repeat_ngram": 3,
       "ngram_repetition_penalty": 1.75}'
```

</TabItem>

<TabItem value="Python">

```python
# Inference request to Running PeriFlow Container using python requests.

import requests
import json

output = requests.post(
    url=url,
    headers=headers,
    json={
        "prompt": "Python is a popular",
        "min_tokens": 20,
        "max_tokens": 30,
        "top_k": 32,
        "top_p": 0.8,
        "n": 3,
        "no_repeat_ngram": 3,
        "ngram_repetition_penalty": 1.75,
    },
)
print(output.json())
```

</TabItem>
</Tabs>
