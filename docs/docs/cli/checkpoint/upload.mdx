# pf checkpoint upload

## Usage

```bash
pf checkpoint upload [OPTIONS]
```

## Summary

Creates a checkpoint by uploading local checkpoint files.

An example of attribute files is as follows.
You have the flexibility to modify the value of each field according to your preferences,
but it is mandatory to fill in every field.

```yaml
# blenderbot
model_type: blenderbot
dtype: fp16
head_size: 80
num_heads: 32
hidden_size: 2560
ff_intermediate_size: 10240
num_encoder_layers: 2
num_decoder_layers: 24
max_input_length: 128
max_output_length: 128
vocab_size: 8008
eos_token: 2
decoder_start_token: 1

# bloom
model_type: bloom
dtype: fp16
head_size: 128
num_heads: 32
num_layers: 30
max_length: 2048
vocab_size: 250880
eos_token: 2

# gpt
model_type: gpt
dtype: fp16
head_size: 64
num_heads: 25
num_layers: 48
max_length: 1024
vocab_size: 50257
eos_token: 50256

# gpt-j
model_type: gpt-j
dtype: fp16
head_size: 256
rotary_dim: 64
num_heads: 16
num_layers: 28
max_length: 2048
vocab_size: 50400
eos_token: 50256

# gpt-neox
model_type: gpt-neox
dtype: fp16
head_size: 128
rotary_dim: 32
num_heads: 40
num_layers: 36
max_length: 2048
vocab_size: 50280
eos_token: 0

# llama
model_type: llama
dtype: fp16
head_size: 128
rotary_dim: 128
num_heads: 32
num_layers: 32
ff_intermediate_size: 11008
max_length: 2048
vocab_size: 32000
eos_token: 1

# opt
model_type: opt
dtype: fp16
head_size: 128
num_heads: 32
num_layers: 32
max_length: 2048
vocab_size: 50272
eos_token: 2

# t5
model_type: t5
dtype: fp16
head_size: 128
num_heads: 32
hidden_size: 1024
ff_intermediate_size: 16384
num_encoder_layers: 24
num_decoder_layers: 24
max_input_length: 512
max_output_length: 512
num_pos_emb_buckets: 32
max_pos_distance: 128
vocab_size: 32100
eos_token: 1
decoder_start_token: 0

# t5-v1_1
model_type: t5-v1_1
dtype: fp16
head_size: 64
num_heads: 32
hidden_size: 2048
ff_intermediate_size: 5120
num_encoder_layers: 24
num_decoder_layers: 24
max_input_length: 512
max_output_length: 512
num_pos_emb_buckets: 32
max_pos_distance: 128
vocab_size: 32128
eos_token: 1
decoder_start_token: 0
```

:::tip
If you want to use tokenizer, `tokenizer.json` file should be in the same directory
as checkpoint file(`*.h5`).
:::

## Options

| Option | Type | Summary | Default | Required |
|--------|------|---------|---------|----------|
| **`--name`**, **`-n`** | TEXT | Name of the checkpoint to upload | - | ✅ |
| **`--source-path`**, **`-p`** | TEXT | Path to source file or directory to upload | - | ✅ |
| `--iteration` | INTEGER | The iteration number of the checkpoint. | None | ❌ |
| `--attr-file`, `-f` | TEXT | Path to the file containing the checkpoint attributes. The file should be in YAML format. | None | ❌ |
| `--max-workers`, `-w` | INTEGER | The number of threads to upload files. | 14 | ❌ |
